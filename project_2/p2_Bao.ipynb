{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python387jvsc74a57bd09ac882c61044a63b565474b83697afffe53c219aaaeb009703caa7c644b95f4f",
   "display_name": "Python 3.8.7 64-bit ('reinforcement_learning')"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the root directory is in Python's path. This is to make importing\n",
    "# custom library easier.\n",
    "from pathlib import Path\n",
    "import sys\n",
    "root = Path('.').absolute().parent.parent\n",
    "if str(root) not in sys.path:\n",
    "    sys.path.append(str(root))\n",
    "\n",
    "# built-in packages\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from typing import List, Dict, Tuple, Callable\n",
    "import math\n",
    "\n",
    "# external packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import scipy\n",
    "import dill\n",
    "\n",
    "# matplotlib param\n",
    "# https://stackoverflow.com/a/55188780/9723036\n",
    "# SMALL_SIZE = 10\n",
    "MEDIUM_SIZE = 15\n",
    "BIGGER_SIZE = 17\n",
    "TEXT_BOTTOM = 0.9\n",
    "\n",
    "plt.rc('font', size=MEDIUM_SIZE)          # controls default text sizes\n",
    "# plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n",
    "# plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.10f}\".format(x)})"
   ]
  },
  {
   "source": [
    "# Initialize Parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS = [[-1, 0], [0, 1], [1, 0], [0, -1]]  # N, W, S, E\n",
    "M = 10  # number of rows\n",
    "N = 10  # number of columns\n",
    "NUM_BLK = 15  # number of blocks\n",
    "GAMMA = 0.9  # discount rate\n",
    "ALPHA = 0.1  # learning rate\n",
    "OPTIMAL_STEPS = 18  # empirically acquired\n",
    "EPSILON = 0.2  # for epsilon-greedy method\n",
    "LAMBDA = 0.8  # for eligibility tracing"
   ]
  },
  {
   "source": [
    "# Shared Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_grid(num_rows: int, num_cols: int, num_blocks: int, random_state: int = 42):\n",
    "    \"\"\"Initialize a grid world, with obstacles.\n",
    "\n",
    "    :param num_rows: Number of rows in the grid world.\n",
    "    :param num_cols: Number of columns in the grid world.\n",
    "    :param num_blocks: Number of RANDOM blocks that agent cannot cross. The final grid world\n",
    "        could include more blocks as additional  a manually \n",
    "    :param random_state: For reproducing random values.\n",
    "    :return: Grid for state values, action values, and the coordinates for blocks.\n",
    "    \"\"\"\n",
    "    # Create a grid\n",
    "    grid = np.array([[0] * num_cols for _ in range(num_rows)])\n",
    "    V = np.array([[0.0] * num_cols for _ in range(num_rows)])  # initial state values\n",
    "    Q = np.array([[[0.0] * len(ACTIONS) for _ in range(num_cols)] for _ in range(num_rows)])  # initial action values\n",
    "    # randomly generate blocks\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    block_i = rng.integers(low=0, high=num_rows, size=num_blocks)\n",
    "    block_j = rng.integers(low=0, high=num_cols, size=num_blocks)\n",
    "    \n",
    "    ###################################################\n",
    "    # hardcodede additional blocks, such that we can  #\n",
    "    # limit the possible number of ways to reach the  #\n",
    "    # terminal state                                  #\n",
    "    ###################################################\n",
    "    block_i = np.append(block_i, [8, 8, 8])\n",
    "    block_j = np.append(block_j, [4, 5, 6])\n",
    "    \n",
    "    # Update grid and state values by identifying the blocks\n",
    "    grid[block_i, block_j] = 1\n",
    "    V[block_i, block_j] = -1000.0\n",
    "\n",
    "    # Update state action values to set the illegal actions to a big negative value\n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_cols):\n",
    "            if i == num_rows - 1 and j == num_cols - 1:  # not touching the terminal state\n",
    "                continue\n",
    "            if grid[i, j] == 1:  # the blocks\n",
    "                Q[i, j] = np.full(4, -1000.0)\n",
    "                continue\n",
    "            for k, (di, dj) in enumerate(ACTIONS):\n",
    "                ni, nj = i + di, j + dj\n",
    "                if not (0 <= ni < num_rows and 0 <= nj < num_cols and grid[ni, nj] == 0):\n",
    "                    Q[i, j, k] = -1000.0\n",
    "\n",
    "    # initialize random policy\n",
    "    pi = np.array([[-1] * num_rows for _ in range(num_cols)])\n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_cols):\n",
    "            if (i == num_rows - 1 and j == num_cols - 1) or grid[i][j]:\n",
    "                continue  # skip terminal state and blocks\n",
    "            pot_actions = []\n",
    "            for idx, (di, dj) in enumerate(ACTIONS):\n",
    "                ni, nj = i + di, j + dj\n",
    "                if 0 <= ni < num_rows and 0 <= nj < num_cols and grid[ni, nj] == 0:\n",
    "                    pot_actions.append(idx)\n",
    "            if pot_actions:\n",
    "                pi[i, j] = rng.choice(pot_actions)\n",
    "    \n",
    "    return np.copy(V), np.copy(Q), grid, np.copy(pi)\n",
    "\n",
    "\n",
    "def get_next_state_vals(i: int, j: int, V, grid, fv: Callable = lambda v: v):\n",
    "    \"\"\"Given the current state i, j, return its next state values.\n",
    "\n",
    "    To be more specific, we return the state values of all possible states\n",
    "    that a next action can lead to, based on the given V. The order of\n",
    "    the next state values is the same as ACTIONS.\n",
    "\n",
    "    :param i: Row index.\n",
    "    :parma j: Column index.\n",
    "    :param V: State values.\n",
    "    :param grid: Grid.\n",
    "    :param fv: A lambda function that takes state value as argument and returns a\n",
    "        new value used for next state values. Default to a lambda that returns the\n",
    "        input state value (i.e., output and input is the same) \n",
    "    :return: A numpy array of shape (4, ) that contains the next state\n",
    "        values. If an action leads to a invalid state, such as out of bound\n",
    "        or hitting a block, its value is set to negative infinite.\n",
    "    \"\"\"\n",
    "    next_state_values = []  # store all Next State ValueS\n",
    "    for di, dj in ACTIONS:\n",
    "        ni, nj = i + di, j + dj\n",
    "        if 0 <= ni < M and 0 <= nj < N and grid[ni, nj] == 0:\n",
    "            next_state_values.append(fv(V[ni, nj]))\n",
    "        else:\n",
    "            next_state_values.append(-math.inf)\n",
    "    return np.array(next_state_values)\n",
    "\n",
    "\n",
    "def find_best_actions(next_state_values) -> List[int]:\n",
    "    \"\"\"Find the best action for an agent based on the given next state values\n",
    "\n",
    "    Note that there could be multiple best actions if multiple next state values\n",
    "    are maximum.\n",
    "\n",
    "    :param next_state_values: A list of state values corresponding to the next\n",
    "        state achieved from taking the actions in ACTIONS. The order of this\n",
    "        list of state values correspond to the order in ACTIONS.\n",
    "    :return: A list of indices correspinding to the best action to take.\n",
    "    \"\"\"\n",
    "    max_V = max(next_state_values)\n",
    "    sorted_indices = np.argsort(np.array(next_state_values))\n",
    "    # find all the potential action indices. This is to handle the situation\n",
    "    # where multiple actions lead to the same next state value. When that\n",
    "    # happens, we randomly pick an action to go.\n",
    "    return [idx for idx in sorted_indices if np.isclose(next_state_values[idx], max_V)]\n",
    "\n",
    "\n",
    "def steps_to_go(V, grid, random_state: int = 42) -> int:\n",
    "    \"\"\"Compute on average the number of steps needed to reach terminal state from start.\n",
    "\n",
    "    Tie breaks on state value is a random choice. We also have a high bound on the number\n",
    "    of steps to take. If the path search results in steps larger than the high bound, it\n",
    "    is very likely that the state values are incomplete and contain loop. In that case,\n",
    "    we exist the path search and directly assign the high bound as the number of steps.\n",
    "\n",
    "    :param V: State values of the grid world.\n",
    "    :param grid: The grid world.\n",
    "    :param random_state: For reproducing random values.\n",
    "    :return: Average number of steps to reach terminal state.\n",
    "    \"\"\"\n",
    "    max_step_allowed = 1000  # if steps count go beyond this value, we terminate the search\n",
    "    m, n = V.shape\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    total_steps = 0\n",
    "    total_iteration = 50  # walk the agent 50 times, take the average steps at the end\n",
    "    for _ in range(total_iteration):\n",
    "        i, j = 0, 0\n",
    "        cur_steps = 0\n",
    "        deterministic = True  # flag to indicate whether a deterministic path can be found\n",
    "        while (i != m - 1 or j != n - 1) and cur_steps < max_step_allowed:\n",
    "            nsvs = get_next_state_vals(i, j, V, grid)\n",
    "            best_acts = find_best_actions(nsvs)\n",
    "            if len(best_acts) > 1:\n",
    "                deterministic = False\n",
    "            next_idx = rng.choice(best_acts)\n",
    "            i, j = i + ACTIONS[next_idx][0], j + ACTIONS[next_idx][1]\n",
    "            cur_steps += 1\n",
    "        if deterministic:  # already deterministic, we can end already\n",
    "            return cur_steps\n",
    "        if cur_steps == max_step_allowed:  # If V hasn't converged, return high bound directy\n",
    "            return cur_steps\n",
    "        total_steps += cur_steps\n",
    "    return total_steps / total_iteration\n",
    "\n",
    "\n",
    "def update_policy(V, pi, grid):\n",
    "    \"\"\"Update the current policy pi given new state values V\n",
    "\n",
    "    :param V: Current state values.\n",
    "    :param pi: previous policy.\n",
    "    :param grid: The grid world.\n",
    "    \"\"\"\n",
    "    m, n = pi.shape\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            if pi[i, j] >= 0:\n",
    "                nsvs = get_next_state_vals(i, j, V, grid)\n",
    "                if nsvs[pi[i, j]] < max(nsvs):  # current policy is not optimal\n",
    "                    pi[i, j] = np.argmax(nsvs)\n",
    "\n",
    "\n",
    "def get_epsilon_greedy_action(Q, i: int, j: int, grid) -> int:\n",
    "    \"\"\"Use epsilon-greedy method to generate an action based on state, action pair.\n",
    "\n",
    "    With epsilon probablity, we exploit. With 1 - epsilon probability, we explore.\n",
    "\n",
    "    :param Q: state, active values.\n",
    "    :param i: Row index of the state.\n",
    "    :param j: Column index of the state.\n",
    "    :param grid: The grid world.\n",
    "    \"\"\"\n",
    "    while True:  # choose a legal action based on epsilon-greedy\n",
    "        method = rng.choice(['explore', 'exploit'], p=[EPSILON, 1 - EPSILON])\n",
    "        if method == 'explore':\n",
    "            act = rng.choice(len(ACTIONS))\n",
    "        else:\n",
    "            best_acts = find_best_actions(Q[i, j])\n",
    "            act = rng.choice(best_acts)\n",
    "        di, dj = ACTIONS[act]\n",
    "        ni, nj = i + di, j + dj\n",
    "        if 0 <= ni < m and 0 <= nj < n and grid[ni, nj] == 0:\n",
    "            return act  # valid action"
   ]
  },
  {
   "source": [
    "# Plot Related"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_step_to_go(ax, steps, optimal_steps: int, title: str):\n",
    "    \"\"\"Plot step-to-go vs. episode\n",
    "\n",
    "    :param ax: An axis object of matplotlib.\n",
    "    :param steps: Number of expected steps to reach the target for each episode.\n",
    "    :param optimal_steps: The optimal number of steps to reach the target.\n",
    "    :param title: Title for the step-to-go plot\n",
    "    \"\"\"\n",
    "    ax.plot(np.arange(1, len(steps) + 1), steps, marker='o', color='C0', label='Steps to Go')\n",
    "    ax.axhline(optimal_steps, color='C2', label='Optimal Steps')\n",
    "    ax.set_xlabel('Episodes')\n",
    "    ax.set_ylabel('Steps')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "def plot_rms_error(ax, rms, title: str):\n",
    "    \"\"\"Plot RMS error vs. episode\n",
    "\n",
    "    :param ax: An axis object of matplotlib.\n",
    "    :param rms: The rms error for each episode.\n",
    "    :param title: Title for the step-to-go plot\n",
    "    \"\"\"\n",
    "    ax.plot(np.arange(1, len(rms) + 1), rms, marker='s', color='C1', label='RMS')\n",
    "    ax.set_xlabel('Episodes')\n",
    "    ax.set_ylabel('RMS')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "def plot_state_value_heatmap(ax, V, fig, title: str, color_vmin: int = -15, color_vmax: int = 0):\n",
    "    \"\"\"Plot state value as a heatmap.\n",
    "\n",
    "    :param ax: An axis object of matplotlib.\n",
    "    :param V: The final estimated state values.\n",
    "    :param fig: The fig parameter, acquired when calling plt.subplots().\n",
    "    :param title: Title for the step-to-go plot.\n",
    "    :param color_vmin: Min value for the color map indicator, default to -15.\n",
    "    :param color_vmax: Max value for the color map indicator, default to 0.\n",
    "    \"\"\"\n",
    "    hm = ax.imshow(\n",
    "        V,\n",
    "        cmap='hot',\n",
    "        interpolation='nearest',\n",
    "        vmin=-15,  # colorbar min\n",
    "        vmax=0,  # colorbar max\n",
    "    )\n",
    "    cbar = fig.colorbar(hm)\n",
    "    cbar.set_label('State Value')\n",
    "    # add V value to each heatmap cell\n",
    "    m, n = V.shape\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            val = '-inf' if V[i, j] <= -1000.0 else f'{V[i, j]:.2f}'\n",
    "            ax.annotate(val, xy=(j - 0.5, i + 0.1), fontsize=10)\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "def plot_path(ax, pi, grid, title: str):\n",
    "    \"\"\"Plot a route from start to end on the grid world.\n",
    "\n",
    "    Yellow is start; green is end; brown is obstacle.\n",
    "\n",
    "    :param ax: An axis object of matplotlib.\n",
    "    :parma pi: The final policy.\n",
    "    :param grid: The grid world.\n",
    "    :param title: Title for the step-to-go plot\n",
    "    \"\"\"\n",
    "    m, n = pi.shape\n",
    "    ax.grid(True)\n",
    "    ax.set_xticks(np.arange(n))\n",
    "    ax.set_xlim(left=0, right=n)\n",
    "    ax.set_yticks(np.arange(m))\n",
    "    ax.set_ylim(top=m - 1, bottom=-1)\n",
    "    ax.invert_yaxis()  # invert y axis such that 0 is at the top\n",
    "    i = j = 0\n",
    "    while i != m - 1 or j != n - 1:\n",
    "        di, dj = ACTIONS[pi[i, j]]\n",
    "        ni, nj = i + di, j + dj\n",
    "        ax.plot([j + 0.5, nj + 0.5], [i - 0.5, ni - 0.5], color='red', lw=2)\n",
    "        i, j = ni, nj\n",
    "    # label start, end and blocks\n",
    "    ax.add_patch(Rectangle((0, -1), 1, 1, fill=True, color='yellow'))\n",
    "    ax.add_patch(Rectangle((n - 1, m - 2), 1, 1, fill=True, color='green'))\n",
    "    for i, j in zip(*np.where(grid == 1)):\n",
    "        ax.add_patch(Rectangle((j, i - 1), 1, 1, fill=True, color='brown'))\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "def plot(steps, rms, V, pi, optimal_steps: int, title: str):\n",
    "    \"\"\"Plot the training data and results.\n",
    "    \n",
    "    This function plots four graphs.\n",
    "    1. Step-to-go vs. episode\n",
    "    2. RMS error vs. episode\n",
    "    3. State value as a heatmap\n",
    "    4. One route found by the reinforcement learning to reach destination.\n",
    "\n",
    "    :param steps: Number of expected steps to reach the target for each episode.\n",
    "    :param rms: The rms error for each episode.\n",
    "    :param V: The final estimated state values.\n",
    "    :parma pi: The final policy\n",
    "    :param optimal_steps: The optimal number of steps to reach the target.\n",
    "    :param title: The title for the plot, also serving as the filename of the saved\n",
    "        plot file.\n",
    "    \"\"\"\n",
    "    plt.rc('grid', linestyle=\"-\", color='black')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(13, 13))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    plot_step_to_go(axes[0], steps, optimal_steps, '(A)')\n",
    "    plot_rms_error(axes[1], rms, '(B)')\n",
    "    plot_path(axes[2], pi, grid, '(C)')\n",
    "    plot_state_value_heatmap(axes[3], V, fig, '(D)')\n",
    "    \n",
    "    fig.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    filename = ''.join(title.split())\n",
    "    plt.savefig(f'{filename}.pdf')"
   ]
  },
  {
   "source": [
    "# Dynamic Programming"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Iterative Policy Evaluation\n",
    "\n",
    "* Random action"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'm' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ce90b5176ac2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdp_it_pol_eval_V\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdp_it_pol_eval_pi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_BLK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mV_pre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdp_it_pol_eval_V\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdp_it_pol_eval_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdp_it_pol_eval_rms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-a950b54860b0>\u001b[0m in \u001b[0;36minitialize_grid\u001b[0;34m(num_rows, num_cols, num_blocks, random_state)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mACTIONS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mni\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mni\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mnj\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mni\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                     \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1000.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'm' is not defined"
     ]
    }
   ],
   "source": [
    "dp_it_pol_eval_V, _, grid, dp_it_pol_eval_pi = initialize_grid(M, N, NUM_BLK, random_state=10)\n",
    "V_pre = np.copy(dp_it_pol_eval_V)\n",
    "dp_it_pol_eval_steps = []\n",
    "dp_it_pol_eval_rms = []\n",
    "for _ in range(30):\n",
    "    # Iterate through ALL states\n",
    "    for i in range(M):\n",
    "        for j in range(N):\n",
    "            if (i == M - 1 and j == N - 1) or grid[i][j]:\n",
    "                continue  # skip terminal state and blocks\n",
    "            num_action_taken = 0\n",
    "            total = 0\n",
    "            for di, dj in ACTIONS:\n",
    "                ni, nj = i + di, j + dj\n",
    "                if 0 <= ni < M and 0 <= nj < N and grid[ni, nj] == 0:\n",
    "                    num_action_taken += 1\n",
    "                    total += (-1 + GAMMA * V_pre[ni, nj])\n",
    "            dp_it_pol_eval_V[i, j] = total / num_action_taken\n",
    "    # Compute error metrics\n",
    "    dp_it_pol_eval_steps.append(steps_to_go(dp_it_pol_eval_V, grid))\n",
    "    dp_it_pol_eval_rms.append(np.linalg.norm(dp_it_pol_eval_V - V_pre) / (M * N))\n",
    "    V_pre = np.copy(dp_it_pol_eval_V)\n",
    "\n",
    "update_policy(dp_it_pol_eval_V, dp_it_pol_eval_pi, grid)\n",
    "dp_it_pol_eval_steps = np.array(dp_it_pol_eval_steps)\n",
    "dp_it_pol_eval_rms = np.array(dp_it_pol_eval_rms)\n",
    "\n",
    "# Pickle everything\n",
    "with open('dp_it_pol_eval_steps.pickle', 'wb') as f_obj:\n",
    "    dill.dump(dp_it_pol_eval_steps, f_obj)\n",
    "with open('dp_it_pol_eval_rms.pickle', 'wb') as f_obj:\n",
    "    dill.dump(dp_it_pol_eval_rms, f_obj)\n",
    "with open('dp_it_pol_eval_V.pickle', 'wb') as f_obj:\n",
    "    dill.dump(dp_it_pol_eval_V, f_obj)\n",
    "with open('dp_it_pol_eval_pi.pickle', 'wb') as f_obj:\n",
    "    dill.dump(dp_it_pol_eval_pi, f_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('dp_it_pol_eval_steps.pickle', 'rb') as f_obj:\n",
    "    dp_it_pol_eval_steps = dill.load(f_obj)\n",
    "with open('dp_it_pol_eval_rms.pickle', 'rb') as f_obj:\n",
    "    dp_it_pol_eval_rms = dill.load(f_obj)\n",
    "with open('dp_it_pol_eval_V.pickle', 'rb') as f_obj:\n",
    "    dp_it_pol_eval_V = dill.load(f_obj)\n",
    "with open('dp_it_pol_eval_pi.pickle', 'rb') as f_obj:\n",
    "    dp_it_pol_eval_pi = dill.load(f_obj)\n",
    "\n",
    "plot(\n",
    "    dp_it_pol_eval_steps,\n",
    "    dp_it_pol_eval_rms,\n",
    "    dp_it_pol_eval_V,\n",
    "    dp_it_pol_eval_pi,\n",
    "    OPTIMAL_STEPS,\n",
    "    'DP - Iterative Policy Evaluation',\n",
    ")\n",
    "print('First episode reach optimal steps', np.where(dp_it_pol_eval_steps == OPTIMAL_STEPS)[0][0])\n",
    "try:\n",
    "    print('First episode converge', np.where(np.isclose(dp_it_pol_eval_rms, 0.0))[0][0])\n",
    "except IndexError:\n",
    "    print(f'RMS not converged yet after {len(dp_it_pol_eval_rms)} episodes')"
   ]
  },
  {
   "source": [
    "## Policy Iteration\n",
    "\n",
    "* Action controlled by policy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_pol_it_V, _, grid, dp_pol_it_pi = initialize_grid(M, N, NUM_BLK, random_state=10)\n",
    "rng = np.random.default_rng(42)\n",
    "V_pre = np.copy(dp_pol_it_V)\n",
    "dp_pol_it_steps = []\n",
    "dp_pol_it_rms = []\n",
    "for _ in range(30):\n",
    "    # Policy Evaluation\n",
    "    for i in range(M):\n",
    "        for j in range(N):\n",
    "            if (i == M - 1 and j == N - 1) or grid[i][j]:\n",
    "                continue  # skip terminal state and blocks\n",
    "            di, dj = ACTIONS[dp_pol_it_pi[i, j]]\n",
    "            dp_pol_it_V[i, j] = -1 + GAMMA * V_pre[i + di, j + dj]\n",
    "    # Compute error metrics\n",
    "    dp_pol_it_steps.append(steps_to_go(dp_pol_it_V, grid))\n",
    "    dp_pol_it_rms.append(np.linalg.norm(dp_pol_it_V - V_pre) / (M * N))\n",
    "    # Policy Improvement\n",
    "    policy_stable = True\n",
    "    for i in range(M):\n",
    "        for j in range(N):\n",
    "            nsvs = get_next_state_vals(i, j, V_pre, grid, fv=lambda v: -1 + GAMMA * v)\n",
    "            best_acts = find_best_actions(nsvs)\n",
    "            A = rng.choice(best_acts)\n",
    "            if A != dp_pol_it_pi[i, j]:\n",
    "                policy_stable = False\n",
    "                dp_pol_it_pi[i, j] = A\n",
    "    if policy_stable:\n",
    "        break\n",
    "    V_pre = np.copy(dp_pol_it_V)\n",
    "\n",
    "dp_pol_it_steps = np.array(dp_pol_it_steps)\n",
    "dp_pol_it_rms = np.array(dp_pol_it_rms)\n",
    "\n",
    "# Pickle everything\n",
    "with open('dp_pol_it_steps.pickle', 'wb') as f_obj:\n",
    "    dill.dump(dp_pol_it_steps, f_obj)\n",
    "with open('dp_pol_it_rms.pickle', 'wb') as f_obj:\n",
    "    dill.dump(dp_pol_it_rms, f_obj)\n",
    "with open('dp_pol_it_V.pickle', 'wb') as f_obj:\n",
    "    dill.dump(dp_pol_it_V, f_obj)\n",
    "with open('dp_pol_it_pi.pickle', 'wb') as f_obj:\n",
    "    dill.dump(dp_pol_it_pi, f_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dp_pol_it_steps.pickle', 'rb') as f_obj:\n",
    "    dp_pol_it_steps = dill.load(f_obj)\n",
    "with open('dp_pol_it_rms.pickle', 'rb') as f_obj:\n",
    "    dp_pol_it_rms = dill.load(f_obj)\n",
    "with open('dp_pol_it_V.pickle', 'rb') as f_obj:\n",
    "    dp_pol_it_V = dill.load(f_obj)\n",
    "with open('dp_pol_it_pi.pickle', 'rb') as f_obj:\n",
    "    dp_pol_it_pi = dill.load(f_obj)\n",
    "\n",
    "plot(\n",
    "    dp_pol_it_steps,\n",
    "    dp_pol_it_rms,\n",
    "    dp_pol_it_V,\n",
    "    dp_pol_it_pi,\n",
    "    OPTIMAL_STEPS,\n",
    "    'DP - Policy Iteration',\n",
    ")\n",
    "print('First episode reach optimal steps', np.where(dp_pol_it_steps == OPTIMAL_STEPS)[0][0])\n",
    "try:\n",
    "    print('First episode converge', np.where(np.isclose(dp_pol_it_rms, 0.0))[0][0])\n",
    "except IndexError:\n",
    "    print(f'RMS not converged yet after {len(dp_pol_it_rms)} episodes')"
   ]
  },
  {
   "source": [
    "## Value Iteration\n",
    "\n",
    "* Action controlled by policy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_val_it_V, _, grid, dp_val_it_pi = initialize_grid(M, N, NUM_BLK, random_state=10)\n",
    "rng = np.random.default_rng(42)\n",
    "V_pre = np.copy(dp_val_it_V)\n",
    "dp_val_it_steps = []\n",
    "dp_val_it_rms = []\n",
    "for _ in range(30):\n",
    "    for i in range(M):\n",
    "        for j in range(N):\n",
    "            if (i == M - 1 and j == N - 1) or grid[i][j]:\n",
    "                continue  # skip terminal state and blocks\n",
    "            nsvs = get_next_state_vals(i, j, V_pre, grid, fv=lambda v: -1 + GAMMA * v)\n",
    "            dp_val_it_V[i, j] = max(nsvs)  # assign best next state value\n",
    "    # Compute error metrics\n",
    "    dp_val_it_steps.append(steps_to_go(dp_val_it_V, grid))\n",
    "    dp_val_it_rms.append(np.linalg.norm(dp_val_it_V - V_pre) / (M * N))\n",
    "    # update V_pre\n",
    "    V_pre = np.copy(dp_val_it_V)\n",
    "\n",
    "update_policy(dp_val_it_V, dp_val_it_pi, grid)\n",
    "dp_val_it_steps = np.array(dp_val_it_steps)\n",
    "dp_val_it_rms = np.array(dp_val_it_rms)\n",
    "\n",
    "# Pickle everything\n",
    "with open('dp_val_it_steps.pickle', 'wb') as f_obj:\n",
    "    dill.dump(dp_val_it_steps, f_obj)\n",
    "with open('dp_val_it_rms.pickle', 'wb') as f_obj:\n",
    "    dill.dump(dp_val_it_rms, f_obj)\n",
    "with open('dp_val_it_V.pickle', 'wb') as f_obj:\n",
    "    dill.dump(dp_val_it_V, f_obj)\n",
    "with open('dp_val_it_pi.pickle', 'wb') as f_obj:\n",
    "    dill.dump(dp_val_it_pi, f_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dp_val_it_steps.pickle', 'rb') as f_obj:\n",
    "    dp_val_it_steps = dill.load(f_obj)\n",
    "with open('dp_val_it_rms.pickle', 'rb') as f_obj:\n",
    "    dp_val_it_rms = dill.load(f_obj)\n",
    "with open('dp_val_it_V.pickle', 'rb') as f_obj:\n",
    "    dp_val_it_V = dill.load(f_obj)\n",
    "with open('dp_val_it_pi.pickle', 'rb') as f_obj:\n",
    "    dp_val_it_pi = dill.load(f_obj)\n",
    "\n",
    "plot(\n",
    "    dp_val_it_steps,\n",
    "    dp_val_it_rms,\n",
    "    dp_val_it_V,\n",
    "    dp_val_it_pi,\n",
    "    OPTIMAL_STEPS,\n",
    "    'DP - Value Iteration',\n",
    ")\n",
    "print('First episode reach optimal steps', np.where(dp_val_it_steps == OPTIMAL_STEPS)[0][0])\n",
    "try:\n",
    "    print('First episode converge', np.where(np.isclose(dp_val_it_rms, 0.0))[0][0])\n",
    "except IndexError:\n",
    "    print(f'RMS not converged yet after {len(dp_val_it_rms)} episodes')"
   ]
  },
  {
   "source": [
    "# Monte Carlo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## First-visit Policy Evalutation\n",
    "\n",
    "* Action completely random\n",
    "* No need to set 1,000-step restriction per episode."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_fst_vst_pol_eval_V, _, grid, mc_fst_vst_pol_eval_pi = initialize_grid(M, N, NUM_BLK, random_state=10)\n",
    "m, n = M, N\n",
    "rng = np.random.default_rng(42)\n",
    "V_pre = np.copy(mc_fst_vst_pol_eval_V)\n",
    "mc_fst_vst_pol_eval_steps = []\n",
    "mc_fst_vst_pol_eval_rms = []\n",
    "# store all the returns encountered at each state\n",
    "returns = [[[] for _ in range(n)] for _ in range(m)]\n",
    "for _ in range(100):\n",
    "    states = []\n",
    "    i, j = 0, 0\n",
    "    while i != m - 1 or j != n - 1:  # One episode\n",
    "        states.append((i, j))\n",
    "        while True:\n",
    "            di, dj = rng.choice(ACTIONS)  # take a random action\n",
    "            ni, nj = i + di, j + dj\n",
    "            if 0 <= ni < m and 0 <= nj < n and grid[ni, nj] == 0:\n",
    "                i, j = ni, nj\n",
    "                break\n",
    "    seen = set()\n",
    "    for k, state in enumerate(states):\n",
    "        if state not in seen:  # only count first visit\n",
    "            seen.add(state)\n",
    "            G = -(GAMMA - GAMMA**(len(states) - k)) / (1 - GAMMA)\n",
    "            returns[state[0]][state[1]].append(G)\n",
    "            mc_fst_vst_pol_eval_V[state[0], state[1]] = np.mean(returns[state[0]][state[1]])\n",
    "    # Compute error metrics\n",
    "    mc_fst_vst_pol_eval_steps.append(steps_to_go(mc_fst_vst_pol_eval_V, grid))\n",
    "    mc_fst_vst_pol_eval_rms.append(np.linalg.norm(mc_fst_vst_pol_eval_V - V_pre) / (M * N))\n",
    "    # update V_pre\n",
    "    V_pre = np.copy(mc_fst_vst_pol_eval_V)\n",
    "\n",
    "update_policy(mc_fst_vst_pol_eval_V, mc_fst_vst_pol_eval_pi, grid)\n",
    "mc_fst_vst_pol_eval_steps = np.array(mc_fst_vst_pol_eval_steps)\n",
    "mc_fst_vst_pol_eval_rms = np.array(mc_fst_vst_pol_eval_rms)\n",
    "\n",
    "# Pickle everything\n",
    "with open('mc_fst_vst_pol_eval_steps.pickle', 'wb') as f_obj:\n",
    "    dill.dump(mc_fst_vst_pol_eval_steps, f_obj)\n",
    "with open('mc_fst_vst_pol_eval_rms.pickle', 'wb') as f_obj:\n",
    "    dill.dump(mc_fst_vst_pol_eval_rms, f_obj)\n",
    "with open('mc_fst_vst_pol_eval_V.pickle', 'wb') as f_obj:\n",
    "    dill.dump(mc_fst_vst_pol_eval_V, f_obj)\n",
    "with open('mc_fst_vst_pol_eval_pi.pickle', 'wb') as f_obj:\n",
    "    dill.dump(mc_fst_vst_pol_eval_pi, f_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mc_fst_vst_pol_eval_steps.pickle', 'rb') as f_obj:\n",
    "    mc_fst_vst_pol_eval_steps = dill.load(f_obj)\n",
    "with open('mc_fst_vst_pol_eval_rms.pickle', 'rb') as f_obj:\n",
    "    mc_fst_vst_pol_eval_rms = dill.load(f_obj)\n",
    "with open('mc_fst_vst_pol_eval_V.pickle', 'rb') as f_obj:\n",
    "    mc_fst_vst_pol_eval_V = dill.load(f_obj)\n",
    "with open('mc_fst_vst_pol_eval_pi.pickle', 'rb') as f_obj:\n",
    "    mc_fst_vst_pol_eval_pi = dill.load(f_obj)\n",
    "\n",
    "plot(\n",
    "    mc_fst_vst_pol_eval_steps,\n",
    "    mc_fst_vst_pol_eval_rms,\n",
    "    mc_fst_vst_pol_eval_V,\n",
    "    mc_fst_vst_pol_eval_pi,\n",
    "    OPTIMAL_STEPS,\n",
    "    'MC - First Visit Policy Evaluation',\n",
    ")\n",
    "print('First episode reach optimal steps', np.where(mc_fst_vst_pol_eval_steps == OPTIMAL_STEPS)[0][0])\n",
    "try:\n",
    "    print('First episode converge', np.where(np.isclose(mc_fst_vst_pol_eval_rms, 0.0))[0][0])\n",
    "except IndexError:\n",
    "    print(f'RMS not converged yet after {len(mc_fst_vst_pol_eval_rms)} episodes')"
   ]
  },
  {
   "source": [
    "## Exploring Starts\n",
    "\n",
    "* Action follows policy\n",
    "* Set 1,000-step restriction per episode in case an episode fails to terminate\n",
    "* Failed episode discarded, i.e. not used to update state or action values.\n",
    "* Initial action value set to a big negative value."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_epl_starts_V, mc_epl_starts_Q, grid, mc_epl_starts_pi = initialize_grid(M, N, NUM_BLK, random_state=10)\n",
    "rng = np.random.default_rng(42)\n",
    "V_pre = np.copy(mc_epl_starts_V)\n",
    "mc_epl_starts_steps = []\n",
    "mc_epl_starts_rms = []\n",
    "m, n = mc_epl_starts_V.shape\n",
    "# store all the returns encountered at each state, action pair\n",
    "returns = [[[[], [], [], []] for _ in range(n)] for _ in range(m)]\n",
    "# Re-initialize Q values\n",
    "for i in range(m):\n",
    "    for j in range(n):\n",
    "        if i == m - 1 and j == n - 1:\n",
    "            continue\n",
    "        mc_epl_starts_Q[i, j] = np.full(len(ACTIONS), -1000.0)\n",
    "\n",
    "for _ in range(600):\n",
    "    states = []\n",
    "    while True:  # random start with state and action\n",
    "        i, j, A = rng.integers(0, m), rng.integers(0, n), rng.integers(0, len(ACTIONS))\n",
    "        di, dj = ACTIONS[A]\n",
    "        ni, nj = i + di, j + dj\n",
    "        if 0 <= ni < m and 0 <= nj < n and grid[ni, nj] == 0 and grid[i, j] == 0:\n",
    "            break\n",
    "    c = 0\n",
    "    max_steps = 1000\n",
    "    while (i != m - 1 or j != n - 1) and c < max_steps:  # One episode\n",
    "        states.append((i, j, A))\n",
    "        i, j = i + ACTIONS[A][0], j + ACTIONS[A][1]\n",
    "        if any(returns[i][j]):  # the next state has been visited before\n",
    "            A = mc_epl_starts_pi[i, j]\n",
    "        else:  # the next state hasn't been visited, use random action\n",
    "            while True:\n",
    "                A = rng.integers(0, len(ACTIONS))\n",
    "                ni, nj = i + ACTIONS[A][0], j + ACTIONS[A][1]\n",
    "                if 0 <= ni < m and 0 <= nj < n and grid[ni, nj] == 0:\n",
    "                    break\n",
    "        c += 1\n",
    "    if c < max_steps:  # Only update state values when an episode reaches terminal state\n",
    "        seen = set()\n",
    "        for k, state in enumerate(states):\n",
    "            if state not in seen:  # only count first visit\n",
    "                seen.add(state)\n",
    "                G = -(GAMMA - GAMMA**(len(states) - k)) / (1 - GAMMA)\n",
    "                returns[state[0]][state[1]][state[2]].append(G)\n",
    "                mc_epl_starts_Q[state[0], state[1], state[2]] = np.mean(returns[state[0]][state[1]][state[2]])\n",
    "        # update V and pi\n",
    "        mc_epl_starts_V = np.max(mc_epl_starts_Q, axis=2)\n",
    "        mc_epl_starts_V[m - 1, n - 1] = 0\n",
    "        update_policy(mc_epl_starts_V, mc_epl_starts_pi, grid)\n",
    "        # Compute error metrics\n",
    "        mc_epl_starts_steps.append(steps_to_go(mc_epl_starts_V, grid))\n",
    "        mc_epl_starts_rms.append(np.linalg.norm(mc_epl_starts_V - V_pre) / (M * N))\n",
    "        # update V_pre\n",
    "        V_pre = np.copy(mc_epl_starts_V)\n",
    "\n",
    "update_policy(mc_epl_starts_V, mc_epl_starts_pi, grid)\n",
    "mc_epl_starts_steps = np.array(mc_epl_starts_steps)\n",
    "mc_epl_starts_rms = np.array(mc_epl_starts_rms)\n",
    "\n",
    "# Pickle everything\n",
    "with open('mc_epl_starts_steps.pickle', 'wb') as f_obj:\n",
    "    dill.dump(mc_epl_starts_steps, f_obj)\n",
    "with open('mc_epl_starts_rms.pickle', 'wb') as f_obj:\n",
    "    dill.dump(mc_epl_starts_rms, f_obj)\n",
    "with open('mc_epl_starts_V.pickle', 'wb') as f_obj:\n",
    "    dill.dump(mc_epl_starts_V, f_obj)\n",
    "with open('mc_epl_starts_pi.pickle', 'wb') as f_obj:\n",
    "    dill.dump(mc_epl_starts_pi, f_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mc_epl_starts_steps.pickle', 'rb') as f_obj:\n",
    "    mc_epl_starts_steps = dill.load(f_obj)\n",
    "with open('mc_epl_starts_rms.pickle', 'rb') as f_obj:\n",
    "    mc_epl_starts_rms = dill.load(f_obj)\n",
    "with open('mc_epl_starts_V.pickle', 'rb') as f_obj:\n",
    "    mc_epl_starts_V = dill.load(f_obj)\n",
    "with open('mc_epl_starts_pi.pickle', 'rb') as f_obj:\n",
    "    mc_epl_starts_pi = dill.load(f_obj)\n",
    "\n",
    "plot(\n",
    "    mc_epl_starts_steps,\n",
    "    mc_epl_starts_rms,\n",
    "    mc_epl_starts_V,\n",
    "    mc_epl_starts_pi,\n",
    "    OPTIMAL_STEPS,\n",
    "    'MC - Exploring Starts',\n",
    ")\n",
    "print('First episode reach optimal steps', np.where(mc_epl_starts_steps == OPTIMAL_STEPS)[0][0])\n",
    "try:\n",
    "    print('First episode converge', np.where(np.isclose(mc_epl_starts_rms, 0.0))[0][0])\n",
    "except IndexError:\n",
    "    print(f'RMS not converged yet after {len(mc_epl_starts_rms)} episodes')"
   ]
  },
  {
   "source": [
    "## On-Policy MC Control\n",
    "\n",
    "* Action follows policy\n",
    "* Set 1,000-step restriction per episode in case an episode fails to terminate\n",
    "* Failed episode discarded, i.e. not used to update state or action values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_on_policy_V, mc_on_policy_Q, grid, mc_on_policy_pi = initialize_grid(M, N, NUM_BLK, random_state=10)\n",
    "rng = np.random.default_rng(42)\n",
    "V_pre = np.copy(mc_on_policy_V)\n",
    "mc_on_policy_steps = []\n",
    "mc_on_policy_rms = []\n",
    "m, n = mc_on_policy_V.shape\n",
    "# store all the returns encountered at each state, action pair\n",
    "returns = [[[[], [], [], []] for _ in range(n)] for _ in range(m)]\n",
    "\n",
    "# set up epsilon-greedy policy. Initial policy as equal probability\n",
    "# for each action\n",
    "epsilon_pi = np.array([[[1 / len(ACTIONS)] * len(ACTIONS) for _ in range(n)] for _ in range(m)])\n",
    "\n",
    "for _ in range(100):\n",
    "    states = []\n",
    "    i, j = 0, 0\n",
    "    c = 0\n",
    "    max_steps = 1000\n",
    "    while (i != m - 1 or j != n - 1) and c < max_steps:  # One episode\n",
    "        while True:\n",
    "            A = rng.choice(len(ACTIONS), p=epsilon_pi[i, j])\n",
    "            di, dj = ACTIONS[A]\n",
    "            ni, nj = i + di, j + dj\n",
    "            if 0 <= ni < m and 0 <= nj < n and grid[ni, nj] == 0:\n",
    "                break  # valid action\n",
    "        states.append((i, j, A))\n",
    "        i, j = ni, nj\n",
    "        c += 1\n",
    "    if c < max_steps:  # only update state-action values when an episode succeeds\n",
    "        seen = set()\n",
    "        for k, state in enumerate(states):\n",
    "            if state not in seen:  # only count first visit\n",
    "                seen.add(state)\n",
    "                G = -(GAMMA - GAMMA**(len(states) - k)) / (1 - GAMMA)\n",
    "                returns[state[0]][state[1]][state[2]].append(G)\n",
    "                mc_on_policy_Q[state[0], state[1], state[2]] = np.mean(returns[state[0]][state[1]][state[2]])\n",
    "        # update V and pi\n",
    "        mc_on_policy_V = np.max(mc_on_policy_Q, axis=2)\n",
    "        mc_on_policy_V[m - 1, n - 1] = 0\n",
    "        update_policy(mc_on_policy_V, mc_on_policy_pi, grid)\n",
    "        # update epsilon_pi\n",
    "        for i, j, _ in states:\n",
    "            A_star = mc_on_policy_pi[i, j]\n",
    "            for k in range(len(ACTIONS)):\n",
    "                if k == A_star:\n",
    "                    epsilon_pi[i, j, k] = 1 - EPSILON + EPSILON / len(ACTIONS)\n",
    "                else:\n",
    "                    epsilon_pi[i, j, k] = EPSILON / len(ACTIONS)\n",
    "\n",
    "        # Compute error metrics\n",
    "        mc_on_policy_steps.append(steps_to_go(mc_on_policy_V, grid))\n",
    "        mc_on_policy_rms.append(np.linalg.norm(mc_on_policy_V - V_pre) / (M * N))\n",
    "        # update V_pre\n",
    "        V_pre = np.copy(mc_on_policy_V)\n",
    "\n",
    "update_policy(mc_on_policy_V, mc_on_policy_pi, grid)\n",
    "mc_on_policy_steps = np.array(mc_on_policy_steps)\n",
    "mc_on_policy_rms = np.array(mc_on_policy_rms)\n",
    "\n",
    "# Pickle everything\n",
    "with open('mc_on_policy_steps.pickle', 'wb') as f_obj:\n",
    "    dill.dump(mc_on_policy_steps, f_obj)\n",
    "with open('mc_on_policy_rms.pickle', 'wb') as f_obj:\n",
    "    dill.dump(mc_on_policy_rms, f_obj)\n",
    "with open('mc_on_policy_V.pickle', 'wb') as f_obj:\n",
    "    dill.dump(mc_on_policy_V, f_obj)\n",
    "with open('mc_on_policy_pi.pickle', 'wb') as f_obj:\n",
    "    dill.dump(mc_on_policy_pi, f_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mc_on_policy_steps.pickle', 'rb') as f_obj:\n",
    "    mc_on_policy_steps = dill.load(f_obj)\n",
    "with open('mc_on_policy_rms.pickle', 'rb') as f_obj:\n",
    "    mc_on_policy_rms = dill.load(f_obj)\n",
    "with open('mc_on_policy_V.pickle', 'rb') as f_obj:\n",
    "    mc_on_policy_V = dill.load(f_obj)\n",
    "with open('mc_on_policy_pi.pickle', 'rb') as f_obj:\n",
    "    mc_on_policy_pi = dill.load(f_obj)\n",
    "\n",
    "plot(\n",
    "    mc_on_policy_steps,\n",
    "    mc_on_policy_rms,\n",
    "    mc_on_policy_V,\n",
    "    mc_on_policy_pi,\n",
    "    OPTIMAL_STEPS,\n",
    "    'MC - On Policy Control',\n",
    ")\n",
    "print('First episode reach optimal steps', np.where(mc_on_policy_steps == OPTIMAL_STEPS)[0][0])\n",
    "try:\n",
    "    print('First episode converge', np.where(np.isclose(mc_on_policy_rms, 0.0))[0][0])\n",
    "except IndexError:\n",
    "    print(f'RMS not converged yet after {len(mc_on_policy_steps)} episodes')"
   ]
  },
  {
   "source": [
    "# Temporal Difference Learning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## TD(0)\n",
    "\n",
    "* Action completely random\n",
    "* No step restriction on episode"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_zero_V, _, grid, td_zero_pi = initialize_grid(M, N, NUM_BLK, random_state=10)\n",
    "rng = np.random.default_rng(42)\n",
    "V_pre = np.copy(td_zero_V)\n",
    "td_zero_steps = []\n",
    "td_zero_rms = []\n",
    "m, n = td_zero_V.shape\n",
    "\n",
    "for _ in range(100):\n",
    "    i, j = 0, 0\n",
    "    while i != m - 1 or j != n - 1:  # One episode\n",
    "        while True:  # take random action\n",
    "            di, dj = rng.choice(ACTIONS)\n",
    "            ni, nj = i + di, j + dj\n",
    "            if 0 <= ni < m and 0 <= nj < n and grid[ni, nj] == 0:\n",
    "                break  # valid action\n",
    "        td_zero_V[i, j] = td_zero_V[i, j] + ALPHA * (-1 + GAMMA * td_zero_V[ni, nj] - td_zero_V[i, j])\n",
    "        i, j = ni, nj\n",
    "    \n",
    "    # Compute error metrics\n",
    "    td_zero_steps.append(steps_to_go(td_zero_V, grid))\n",
    "    td_zero_rms.append(np.linalg.norm(td_zero_V - V_pre) / (M * N))\n",
    "    # update V_pre\n",
    "    V_pre = np.copy(td_zero_V)\n",
    "\n",
    "update_policy(td_zero_V, td_zero_pi, grid)\n",
    "td_zero_steps = np.array(td_zero_steps)\n",
    "td_zero_rms = np.array(td_zero_rms)\n",
    "\n",
    "# Pickle everything\n",
    "with open('td_zero_steps.pickle', 'wb') as f_obj:\n",
    "    dill.dump(td_zero_steps, f_obj)\n",
    "with open('td_zero_rms.pickle', 'wb') as f_obj:\n",
    "    dill.dump(td_zero_rms, f_obj)\n",
    "with open('td_zero_V.pickle', 'wb') as f_obj:\n",
    "    dill.dump(td_zero_V, f_obj)\n",
    "with open('td_zero_pi.pickle', 'wb') as f_obj:\n",
    "    dill.dump(td_zero_pi, f_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('td_zero_steps.pickle', 'rb') as f_obj:\n",
    "    td_zero_steps = dill.load(f_obj)\n",
    "with open('td_zero_rms.pickle', 'rb') as f_obj:\n",
    "    td_zero_rms = dill.load(f_obj)\n",
    "with open('td_zero_V.pickle', 'rb') as f_obj:\n",
    "    td_zero_V = dill.load(f_obj)\n",
    "with open('td_zero_pi.pickle', 'rb') as f_obj:\n",
    "    td_zero_pi = dill.load(f_obj)\n",
    "\n",
    "plot(\n",
    "    td_zero_steps,\n",
    "    td_zero_rms,\n",
    "    td_zero_V,\n",
    "    td_zero_pi,\n",
    "    OPTIMAL_STEPS,\n",
    "    'TD - Zero',\n",
    ")\n",
    "print('First episode reach optimal steps', np.where(td_zero_steps == OPTIMAL_STEPS)[0][0])\n",
    "try:\n",
    "    print('First episode converge', np.where(np.isclose(td_zero_rms, 0.0))[0][0])\n",
    "except IndexError:\n",
    "    print(f'RMS not converged yet after {len(td_zero_steps)} episodes')"
   ]
  },
  {
   "source": [
    "## Sarsa with TD Control\n",
    "\n",
    "* Action follow policy using $\\epsilon$-greedy\n",
    "* No step restriction on episode"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa_td_V, sarsa_td_Q, grid, sarsa_td_pi = initialize_grid(M, N, NUM_BLK, random_state=10)\n",
    "rng = np.random.default_rng(42)\n",
    "V_pre = np.copy(sarsa_td_V)\n",
    "sarsa_td_steps = []\n",
    "sarsa_td_rms = []\n",
    "m, n = sarsa_td_V.shape\n",
    "\n",
    "for _ in range(200):\n",
    "    i, j = 0, 0\n",
    "    A = get_epsilon_greedy_action(sarsa_td_Q, i, j, grid)\n",
    "    while i != m - 1 or j != n - 1:  # One episode\n",
    "        ni, nj = i + ACTIONS[A][0], j + ACTIONS[A][1]\n",
    "        A_prime = get_epsilon_greedy_action(sarsa_td_Q, ni, nj, grid)\n",
    "        sarsa_td_Q[i, j, A] = sarsa_td_Q[i, j, A] + ALPHA * (-1 + GAMMA * sarsa_td_Q[ni, nj, A_prime] - sarsa_td_Q[i, j, A])\n",
    "        i, j = ni, nj\n",
    "        A = A_prime\n",
    "\n",
    "    # update V\n",
    "    sarsa_td_V = np.max(sarsa_td_Q, axis=2)\n",
    "    \n",
    "    # Compute error metrics\n",
    "    sarsa_td_steps.append(steps_to_go(sarsa_td_V, grid))\n",
    "    sarsa_td_rms.append(np.linalg.norm(sarsa_td_V - V_pre) / (M * N))\n",
    "    # update V_pre\n",
    "    V_pre = np.copy(sarsa_td_V)\n",
    "\n",
    "update_policy(sarsa_td_V, sarsa_td_pi, grid)\n",
    "sarsa_td_steps = np.array(sarsa_td_steps)\n",
    "sarsa_td_rms = np.array(sarsa_td_rms)\n",
    "\n",
    "# Pickle everything\n",
    "with open('sarsa_td_steps.pickle', 'wb') as f_obj:\n",
    "    dill.dump(sarsa_td_steps, f_obj)\n",
    "with open('sarsa_td_rms.pickle', 'wb') as f_obj:\n",
    "    dill.dump(sarsa_td_rms, f_obj)\n",
    "with open('sarsa_td_V.pickle', 'wb') as f_obj:\n",
    "    dill.dump(sarsa_td_V, f_obj)\n",
    "with open('sarsa_td_pi.pickle', 'wb') as f_obj:\n",
    "    dill.dump(sarsa_td_pi, f_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sarsa_td_steps.pickle', 'rb') as f_obj:\n",
    "    sarsa_td_steps = dill.load(f_obj)\n",
    "with open('sarsa_td_rms.pickle', 'rb') as f_obj:\n",
    "    sarsa_td_rms = dill.load(f_obj)\n",
    "with open('sarsa_td_V.pickle', 'rb') as f_obj:\n",
    "    sarsa_td_V = dill.load(f_obj)\n",
    "with open('sarsa_td_pi.pickle', 'rb') as f_obj:\n",
    "    sarsa_td_pi = dill.load(f_obj)\n",
    "\n",
    "plot(\n",
    "    sarsa_td_steps,\n",
    "    sarsa_td_rms,\n",
    "    sarsa_td_V,\n",
    "    sarsa_td_pi,\n",
    "    OPTIMAL_STEPS,\n",
    "    'Sarsa - TD Control',\n",
    ")\n",
    "print('First episode reach optimal steps', np.where(sarsa_td_steps == OPTIMAL_STEPS)[0][0])\n",
    "try:\n",
    "    print('First episode converge', np.where(np.isclose(sarsa_td_rms, 0.0))[0][0])\n",
    "except IndexError:\n",
    "    print(f'RMS not converged yet after {len(sarsa_td_steps)} episodes')"
   ]
  },
  {
   "source": [
    "## Q-learning with TD Control\n",
    "\n",
    "* Action follow policy using $\\epsilon$-greedy\n",
    "* No step restriction on episode"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learn_td_V, q_learn_td_Q, grid, q_learn_td_pi = initialize_grid(M, N, NUM_BLK, random_state=10)\n",
    "rng = np.random.default_rng(42)\n",
    "V_pre = np.copy(q_learn_td_V)\n",
    "q_learn_td_steps = []\n",
    "q_learn_td_rms = []\n",
    "m, n = q_learn_td_V.shape\n",
    "\n",
    "for _ in range(100):\n",
    "    i, j = 0, 0\n",
    "    while i != m - 1 or j != n - 1:  # One episode\n",
    "        A = get_epsilon_greedy_action(q_learn_td_Q, i, j, grid)\n",
    "        ni, nj = i + ACTIONS[A][0], j + ACTIONS[A][1]\n",
    "        q_learn_td_Q[i, j, A] = q_learn_td_Q[i, j, A] + ALPHA * (-1 + GAMMA * max(q_learn_td_Q[ni, nj]) - q_learn_td_Q[i, j, A])\n",
    "        i, j = ni, nj\n",
    "\n",
    "    # update V\n",
    "    q_learn_td_V = np.max(q_learn_td_Q, axis=2)\n",
    "    \n",
    "    # Compute error metrics\n",
    "    q_learn_td_steps.append(steps_to_go(q_learn_td_V, grid))\n",
    "    q_learn_td_rms.append(np.linalg.norm(q_learn_td_V - V_pre) / (M * N))\n",
    "    # update V_pre\n",
    "    V_pre = np.copy(q_learn_td_V)\n",
    "\n",
    "update_policy(q_learn_td_V, q_learn_td_pi, grid)\n",
    "q_learn_td_steps = np.array(q_learn_td_steps)\n",
    "q_learn_td_rms = np.array(q_learn_td_rms)\n",
    "\n",
    "# Pickle everything\n",
    "with open('q_learn_td_steps.pickle', 'wb') as f_obj:\n",
    "    dill.dump(q_learn_td_steps, f_obj)\n",
    "with open('q_learn_td_rms.pickle', 'wb') as f_obj:\n",
    "    dill.dump(q_learn_td_rms, f_obj)\n",
    "with open('q_learn_td_V.pickle', 'wb') as f_obj:\n",
    "    dill.dump(q_learn_td_V, f_obj)\n",
    "with open('q_learn_td_pi.pickle', 'wb') as f_obj:\n",
    "    dill.dump(q_learn_td_pi, f_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('q_learn_td_steps.pickle', 'rb') as f_obj:\n",
    "    q_learn_td_steps = dill.load(f_obj)\n",
    "with open('q_learn_td_rms.pickle', 'rb') as f_obj:\n",
    "    q_learn_td_rms = dill.load(f_obj)\n",
    "with open('q_learn_td_V.pickle', 'rb') as f_obj:\n",
    "    q_learn_td_V = dill.load(f_obj)\n",
    "with open('q_learn_td_pi.pickle', 'rb') as f_obj:\n",
    "    q_learn_td_pi = dill.load(f_obj)\n",
    "\n",
    "plot(\n",
    "    q_learn_td_steps,\n",
    "    q_learn_td_rms,\n",
    "    q_learn_td_V,\n",
    "    q_learn_td_pi,\n",
    "    OPTIMAL_STEPS,\n",
    "    'Q-learning - TD Control',\n",
    ")\n",
    "print('First episode reach optimal steps', np.where(q_learn_td_steps == OPTIMAL_STEPS)[0][0])\n",
    "try:\n",
    "    print('First episode converge', np.where(np.isclose(q_learn_td_rms, 0.0))[0][0])\n",
    "except IndexError:\n",
    "    print(f'RMS not converged yet after {len(q_learn_td_steps)} episodes')"
   ]
  },
  {
   "source": [
    "# Eligibility Tracing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## TD($\\lambda$)\n",
    "\n",
    "* Action completely random\n",
    "* No step restriction on episode\n",
    "* Use accumulating traces"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_lambda_V, _, grid, td_lambda_pi = initialize_grid(M, N, NUM_BLK, random_state=10)\n",
    "rng = np.random.default_rng(42)\n",
    "V_pre = np.copy(td_lambda_V)\n",
    "td_lambda_steps = []\n",
    "td_lambda_rms = []\n",
    "m, n = td_lambda_V.shape\n",
    "\n",
    "for _ in range(100):\n",
    "    # Initialize eligibility tracing\n",
    "    E = np.array([[0.0] * n for _ in range(m)])\n",
    "    i, j = 0, 0\n",
    "    while i != m - 1 or j != n - 1:  # One episode\n",
    "        while True:  # take random action\n",
    "            di, dj = rng.choice(ACTIONS)\n",
    "            ni, nj = i + di, j + dj\n",
    "            if 0 <= ni < m and 0 <= nj < n and grid[ni, nj] == 0:\n",
    "                break  # valid action\n",
    "        delta = -1 + GAMMA * td_lambda_V[ni, nj] - td_lambda_V[i, j]\n",
    "        E[i, j] += 1\n",
    "        for p in range(m):\n",
    "            for q in range(n):\n",
    "                # value update based on eligibility tracing\n",
    "                td_lambda_V[p, q] += ALPHA * delta * E[p, q]\n",
    "                # Attenuation of eligibility tracing\n",
    "                E[p, q] *= LAMBDA * GAMMA\n",
    "        i, j = ni, nj\n",
    "    \n",
    "    # Compute error metrics\n",
    "    td_lambda_steps.append(steps_to_go(td_lambda_V, grid))\n",
    "    td_lambda_rms.append(np.linalg.norm(td_lambda_V - V_pre) / (M * N))\n",
    "    # update V_pre\n",
    "    V_pre = np.copy(td_lambda_V)\n",
    "\n",
    "update_policy(td_lambda_V, td_lambda_pi, grid)\n",
    "td_lambda_steps = np.array(td_lambda_steps)\n",
    "td_lambda_rms = np.array(td_lambda_rms)\n",
    "\n",
    "# Pickle everything\n",
    "with open('td_lambda_steps.pickle', 'wb') as f_obj:\n",
    "    dill.dump(td_lambda_steps, f_obj)\n",
    "with open('td_lambda_rms.pickle', 'wb') as f_obj:\n",
    "    dill.dump(td_lambda_rms, f_obj)\n",
    "with open('td_lambda_V.pickle', 'wb') as f_obj:\n",
    "    dill.dump(td_lambda_V, f_obj)\n",
    "with open('td_lambda_pi.pickle', 'wb') as f_obj:\n",
    "    dill.dump(td_lambda_pi, f_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('td_lambda_steps.pickle', 'rb') as f_obj:\n",
    "    td_lambda_steps = dill.load(f_obj)\n",
    "with open('td_lambda_rms.pickle', 'rb') as f_obj:\n",
    "    td_lambda_rms = dill.load(f_obj)\n",
    "with open('td_lambda_V.pickle', 'rb') as f_obj:\n",
    "    td_lambda_V = dill.load(f_obj)\n",
    "with open('td_lambda_pi.pickle', 'rb') as f_obj:\n",
    "    td_lambda_pi = dill.load(f_obj)\n",
    "\n",
    "plot(\n",
    "    td_lambda_steps,\n",
    "    td_lambda_rms,\n",
    "    td_lambda_V,\n",
    "    td_lambda_pi,\n",
    "    OPTIMAL_STEPS,\n",
    "    'TD - Lambda',\n",
    ")\n",
    "print('First episode reach optimal steps', np.where(td_lambda_steps == OPTIMAL_STEPS)[0][0])\n",
    "try:\n",
    "    print('First episode converge', np.where(np.isclose(td_lambda_rms, 0.0))[0][0])\n",
    "except IndexError:\n",
    "    print(f'RMS not converged yet after {len(td_lambda_steps)} episodes')"
   ]
  },
  {
   "source": [
    "## Sarsa($\\lambda$)\n",
    "\n",
    "* Action follow policy using $\\epsilon$-greedy\n",
    "* No step restriction on episode\n",
    "* Use accumulating traces"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa_lambda_V, sarsa_lambda_Q, grid, sarsa_lambda_pi = initialize_grid(M, N, NUM_BLK, random_state=10)\n",
    "rng = np.random.default_rng(42)\n",
    "V_pre = np.copy(sarsa_lambda_V)\n",
    "sarsa_lambda_steps = []\n",
    "sarsa_lambda_rms = []\n",
    "m, n = sarsa_lambda_V.shape\n",
    "\n",
    "for _ in range(100):\n",
    "    # Initialize eligibility tracing\n",
    "    E = np.array([[[0.0] * len(ACTIONS) for _ in range(n)] for _ in range(m)])\n",
    "    i, j = 0, 0\n",
    "    A = get_epsilon_greedy_action(sarsa_lambda_Q, i, j, grid)\n",
    "    while i != m - 1 or j != n - 1:  # One episode\n",
    "        ni, nj = i + ACTIONS[A][0], j + ACTIONS[A][1]\n",
    "        A_prime = get_epsilon_greedy_action(sarsa_lambda_Q, ni, nj, grid)\n",
    "        delta = -1 + GAMMA * sarsa_lambda_Q[ni, nj, A_prime] - sarsa_lambda_Q[i, j, A]\n",
    "        E[i, j, A] += 1  # accumulating traces\n",
    "        for p in range(m):\n",
    "            for q in range(n):\n",
    "                for k in range(len(ACTIONS)):\n",
    "                    sarsa_lambda_Q[p, q, k] += ALPHA * delta * E[p, q, k]\n",
    "                    E[p, q, k] *= LAMBDA * GAMMA  # attenuation\n",
    "        i, j = ni, nj\n",
    "        A = A_prime\n",
    "\n",
    "    # update V\n",
    "    sarsa_lambda_V = np.max(sarsa_lambda_Q, axis=2)\n",
    "    \n",
    "    # Compute error metrics\n",
    "    sarsa_lambda_steps.append(steps_to_go(sarsa_lambda_V, grid))\n",
    "    sarsa_lambda_rms.append(np.linalg.norm(sarsa_lambda_V - V_pre) / (M * N))\n",
    "    # update V_pre\n",
    "    V_pre = np.copy(sarsa_lambda_V)\n",
    "\n",
    "update_policy(sarsa_lambda_V, sarsa_lambda_pi, grid)\n",
    "sarsa_lambda_steps = np.array(sarsa_lambda_steps)\n",
    "sarsa_lambda_rms = np.array(sarsa_lambda_rms)\n",
    "\n",
    "# Pickle everything\n",
    "with open('sarsa_lambda_steps.pickle', 'wb') as f_obj:\n",
    "    dill.dump(sarsa_lambda_steps, f_obj)\n",
    "with open('sarsa_lambda_rms.pickle', 'wb') as f_obj:\n",
    "    dill.dump(sarsa_lambda_rms, f_obj)\n",
    "with open('sarsa_lambda_V.pickle', 'wb') as f_obj:\n",
    "    dill.dump(sarsa_lambda_V, f_obj)\n",
    "with open('sarsa_lambda_pi.pickle', 'wb') as f_obj:\n",
    "    dill.dump(sarsa_lambda_pi, f_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sarsa_lambda_steps.pickle', 'rb') as f_obj:\n",
    "    sarsa_lambda_steps = dill.load(f_obj)\n",
    "with open('sarsa_lambda_rms.pickle', 'rb') as f_obj:\n",
    "    sarsa_lambda_rms = dill.load(f_obj)\n",
    "with open('sarsa_lambda_V.pickle', 'rb') as f_obj:\n",
    "    sarsa_lambda_V = dill.load(f_obj)\n",
    "with open('sarsa_lambda_pi.pickle', 'rb') as f_obj:\n",
    "    sarsa_lambda_pi = dill.load(f_obj)\n",
    "\n",
    "plot(\n",
    "    sarsa_lambda_steps,\n",
    "    sarsa_lambda_rms,\n",
    "    sarsa_lambda_V,\n",
    "    sarsa_lambda_pi,\n",
    "    OPTIMAL_STEPS,\n",
    "    'Sarsa - Lambda',\n",
    ")\n",
    "print('First episode reach optimal steps', np.where(sarsa_lambda_steps == OPTIMAL_STEPS)[0][0])\n",
    "try:\n",
    "    print('First episode converge', np.where(np.isclose(sarsa_lambda_rms, 0.0))[0][0])\n",
    "except IndexError:\n",
    "    print(f'RMS not converged yet after {len(sarsa_lambda_steps)} episodes')"
   ]
  },
  {
   "source": [
    "## Watkin's Q($\\lambda$)\n",
    "\n",
    "* Action follow policy using $\\epsilon$-greedy\n",
    "* No step restriction on episode\n",
    "* Use accumulating traces"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watkins_q_lambda_V, watkins_q_lambda_Q, grid, watkins_q_lambda_pi = initialize_grid(M, N, NUM_BLK, random_state=10)\n",
    "rng = np.random.default_rng(42)\n",
    "V_pre = np.copy(watkins_q_lambda_V)\n",
    "watkins_q_lambda_steps = []\n",
    "watkins_q_lambda_rms = []\n",
    "m, n = watkins_q_lambda_V.shape\n",
    "\n",
    "for _ in range(100):\n",
    "    # Initialize eligibility tracing\n",
    "    E = np.array([[[0.0] * len(ACTIONS) for _ in range(n)] for _ in range(m)])\n",
    "    i, j = 0, 0\n",
    "    A = get_epsilon_greedy_action(watkins_q_lambda_Q, i, j, grid)\n",
    "    while i != m - 1 or j != n - 1:  # One episode\n",
    "        ni, nj = i + ACTIONS[A][0], j + ACTIONS[A][1]\n",
    "        A_prime = get_epsilon_greedy_action(watkins_q_lambda_Q, ni, nj, grid)\n",
    "        A_star = np.argmax(watkins_q_lambda_Q[ni, nj])\n",
    "        if watkins_q_lambda_Q[ni, nj, A_prime] == watkins_q_lambda_Q[ni, nj, A_star]:\n",
    "            A_star = A_prime\n",
    "        delta = -1 + GAMMA * watkins_q_lambda_Q[ni, nj, A_star] - watkins_q_lambda_Q[i, j, A]\n",
    "        E[i, j, A] += 1  # accumulating traces\n",
    "        for p in range(m):\n",
    "            for q in range(n):\n",
    "                for k in range(len(ACTIONS)):\n",
    "                    watkins_q_lambda_Q[p, q, k] += ALPHA * delta * E[p, q, k]\n",
    "                    if k == A_star:\n",
    "                        E[p, q, k] *= LAMBDA * GAMMA  # attenuation\n",
    "                    else:  # action not greedy, thus its error shall not be highly restricted\n",
    "                        E[p, q, k] = 0\n",
    "        i, j = ni, nj\n",
    "        A = A_prime\n",
    "\n",
    "    # update V\n",
    "    watkins_q_lambda_V = np.max(watkins_q_lambda_Q, axis=2)\n",
    "    \n",
    "    # Compute error metrics\n",
    "    watkins_q_lambda_steps.append(steps_to_go(watkins_q_lambda_V, grid))\n",
    "    watkins_q_lambda_rms.append(np.linalg.norm(watkins_q_lambda_V - V_pre) / (M * N))\n",
    "    # update V_pre\n",
    "    V_pre = np.copy(watkins_q_lambda_V)\n",
    "\n",
    "update_policy(watkins_q_lambda_V, watkins_q_lambda_pi, grid)\n",
    "watkins_q_lambda_steps = np.array(watkins_q_lambda_steps)\n",
    "watkins_q_lambda_rms = np.array(watkins_q_lambda_rms)\n",
    "\n",
    "# Pickle everything\n",
    "with open('watkins_q_lambda_steps.pickle', 'wb') as f_obj:\n",
    "    dill.dump(watkins_q_lambda_steps, f_obj)\n",
    "with open('watkins_q_lambda_rms.pickle', 'wb') as f_obj:\n",
    "    dill.dump(watkins_q_lambda_rms, f_obj)\n",
    "with open('watkins_q_lambda_V.pickle', 'wb') as f_obj:\n",
    "    dill.dump(watkins_q_lambda_V, f_obj)\n",
    "with open('watkins_q_lambda_pi.pickle', 'wb') as f_obj:\n",
    "    dill.dump(watkins_q_lambda_pi, f_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('watkins_q_lambda_steps.pickle', 'rb') as f_obj:\n",
    "    watkins_q_lambda_steps = dill.load(f_obj)\n",
    "with open('watkins_q_lambda_rms.pickle', 'rb') as f_obj:\n",
    "    watkins_q_lambda_rms = dill.load(f_obj)\n",
    "with open('watkins_q_lambda_V.pickle', 'rb') as f_obj:\n",
    "    watkins_q_lambda_V = dill.load(f_obj)\n",
    "with open('watkins_q_lambda_pi.pickle', 'rb') as f_obj:\n",
    "    watkins_q_lambda_pi = dill.load(f_obj)\n",
    "\n",
    "plot(\n",
    "    watkins_q_lambda_steps,\n",
    "    watkins_q_lambda_rms,\n",
    "    watkins_q_lambda_V,\n",
    "    watkins_q_lambda_pi,\n",
    "    OPTIMAL_STEPS,\n",
    "    'Watkins - Q Lambda',\n",
    ")\n",
    "print('First episode reach optimal steps', np.where(watkins_q_lambda_steps == OPTIMAL_STEPS)[0][0])\n",
    "try:\n",
    "    print('First episode converge', np.where(np.isclose(watkins_q_lambda_rms, 0.0))[0][0])\n",
    "except IndexError:\n",
    "    print(f'RMS not converged yet after {len(watkins_q_lambda_steps)} episodes')"
   ]
  },
  {
   "source": [
    "# Draw Grid World without Path"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, grid, _ = initialize_grid(M, N, NUM_BLK, random_state=10)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "ax.grid(True)\n",
    "ax.set_xticks(np.arange(N))\n",
    "ax.set_xlim(left=0, right=N)\n",
    "ax.set_yticks(np.arange(M))\n",
    "ax.set_ylim(top=M - 1, bottom=-1)\n",
    "ax.invert_yaxis()  # invert y axis such that 0 is at the top\n",
    "\n",
    "# label start, end and blocks\n",
    "ax.add_patch(Rectangle((0, -1), 1, 1, fill=True, color='yellow'))\n",
    "ax.annotate(xy=(0.2, -0.4), text='Start')\n",
    "ax.add_patch(Rectangle((n - 1, m - 2), 1, 1, fill=True, color='green'))\n",
    "ax.annotate(xy=(n - 1 + 0.2, m - 1.4), text='End')\n",
    "for i, j in zip(*np.where(grid == 1)):\n",
    "    ax.add_patch(Rectangle((j, i - 1), 1, 1, fill=True, color='brown'))\n",
    "\n",
    "plt.savefig('grid_world.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
