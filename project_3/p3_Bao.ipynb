{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python387jvsc74a57bd09ac882c61044a63b565474b83697afffe53c219aaaeb009703caa7c644b95f4f",
   "display_name": "Python 3.8.7 64-bit ('reinforcement_learning')"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the root directory is in Python's path. This is to make importing\n",
    "# custom library easier.\n",
    "from pathlib import Path\n",
    "import sys\n",
    "root = Path('.').absolute().parent.parent\n",
    "if str(root) not in sys.path:\n",
    "    sys.path.append(str(root))\n",
    "\n",
    "# built-in packages\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from typing import List, Dict, Tuple, Callable\n",
    "import math\n",
    "\n",
    "# external packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import scipy\n",
    "import dill\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# matplotlib param\n",
    "# https://stackoverflow.com/a/55188780/9723036\n",
    "# SMALL_SIZE = 10\n",
    "MEDIUM_SIZE = 15\n",
    "BIGGER_SIZE = 17\n",
    "TEXT_BOTTOM = 0.9\n",
    "\n",
    "plt.rc('font', size=MEDIUM_SIZE)          # controls default text sizes\n",
    "# plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n",
    "# plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.5f}\".format(x)})"
   ]
  },
  {
   "source": [
    "# Initialize Parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS = [[-1, 0], [0, 1], [1, 0], [0, -1]]  # N, E, S, W\n",
    "M = 10  # number of rows\n",
    "N = 10  # number of columns\n",
    "NUM_BLK = 15  # number of blocks\n",
    "GAMMA = 0.9  # discount rate\n",
    "ACTOR_OPTIMIZER = keras.optimizers.SGD(learning_rate=0.01)\n",
    "CRITIC_OPTIMIZER = keras.optimizers.SGD(learning_rate=0.01)\n",
    "OPTIMAL_STEPS = 18  # empirically acquired\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "source": [
    "# TensorFlow Models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_WEIGHT = keras.initializers.RandomNormal(seed=RANDOM_SEED)\n",
    "\n",
    "\n",
    "def state_value_func(hidden_node: int, num_layers: int):\n",
    "    \"\"\"Neural network model simulating the state value function.\n",
    "\n",
    "    Note that a state value function takes a two dimensional input which represents\n",
    "    the current position of the cell in the grid world. For instance, given a cell\n",
    "    (i, j), the cell's state value is\n",
    "\n",
    "    state_value_model(np.array[[i, j]])\n",
    "\n",
    "    The return value is a tensor like this [[state_val]]. To access the value itself,\n",
    "    we can do:\n",
    "\n",
    "    state_value_model(np.array[[i, j]])[0, 0]\n",
    "\n",
    "    :param hidden_node: The number of nodes in each hidden layer.\n",
    "    :param num_layers: The number of hidden layers.\n",
    "    :return: A keras NN model.\n",
    "    \"\"\"\n",
    "    inputs = keras.Input(shape=(2,))\n",
    "    x = layers.Dense(hidden_node, activation='relu', kernel_initializer=INIT_WEIGHT)(inputs)\n",
    "    for _ in range(num_layers - 1):\n",
    "        x = layers.Dense(hidden_node, activation='relu', kernel_initializer=INIT_WEIGHT)(x)\n",
    "    outputs = -layers.Dense(1, activation='relu', kernel_initializer=INIT_WEIGHT)(x)\n",
    "    return keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "def action_value_func(hidden_node: int, num_layers: int):\n",
    "    \"\"\"Neural network model simulating the state value function.\n",
    "\n",
    "    Note that a state value function takes a two dimensional input which represents\n",
    "    the current position of the cell in the grid world. For instance, given a cell\n",
    "    (i, j), the cell's state value is\n",
    "\n",
    "    state_value_model(np.array[[i, j]])\n",
    "\n",
    "    The return value is a tensor like this [[state_val]]. To access the value itself,\n",
    "    we can do:\n",
    "\n",
    "    state_value_model(np.array[[i, j]])[0, 0]\n",
    "\n",
    "    :param hidden_node: The number of nodes in each hidden layer.\n",
    "    :param num_layers: The number of hidden layers.\n",
    "    :return: A keras NN model.\n",
    "    \"\"\"\n",
    "    inputs = keras.Input(shape=(3,))\n",
    "    x = layers.Dense(hidden_node, activation='relu', kernel_initializer=INIT_WEIGHT)(inputs)\n",
    "    for _ in range(num_layers - 1):\n",
    "        x = layers.Dense(hidden_node, activation='relu', kernel_initializer=INIT_WEIGHT)(x)\n",
    "    outputs = -layers.Dense(1, activation='relu', kernel_initializer=INIT_WEIGHT)(x)\n",
    "    return keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "def policy_func(hidden_node: int, num_layers: int, num_actions: int):\n",
    "    \"\"\"Neural network model simulating the policy function.\n",
    "\n",
    "    Note that a policy function takes a two dimensional input which represents the\n",
    "    current position of the cell in the grid world. For instance, given a cell\n",
    "    (i, j), to obtain the policy distribution, we can call\n",
    "    \n",
    "    policy_model(np.array([[i, j]]))\n",
    "\n",
    "    The return value of the above call is a tensor like this [[prob_0, prob_1, prob_2, prob_3]].\n",
    "    To access the probability of an action with index a, we can d0:\n",
    "\n",
    "    policy_model(np.array([[i, j]]))[0, a]\n",
    "\n",
    "    :param hidden_node: The number of nodes in each hidden layer.\n",
    "    :param num_layers: The number of hidden layers.\n",
    "    :return: A keras NN model.\n",
    "    \"\"\"\n",
    "    inputs = keras.Input(shape=(2,))\n",
    "    x = layers.Dense(hidden_node, activation='relu', kernel_initializer=INIT_WEIGHT)(inputs)\n",
    "    for _ in range(num_layers - 1):\n",
    "        x = layers.Dense(hidden_node, activation='relu', kernel_initializer=INIT_WEIGHT)(x)\n",
    "    outputs = layers.Dense(num_actions, activation='softmax', kernel_initializer=INIT_WEIGHT)(x)\n",
    "    return keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "source": [
    "# Plot Related"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_step_to_go(ax, steps, optimal_steps: int, max_episodes, title: str):\n",
    "    \"\"\"Plot step-to-go vs. episode\n",
    "\n",
    "    :param ax: An axis object of matplotlib.\n",
    "    :param steps: Number of expected steps to reach the target for each episode.\n",
    "    :param optimal_steps: The optimal number of steps to reach the target.\n",
    "    :param max_episodes: The maximum number of episodes.\n",
    "    :param title: Title for the step-to-go plot\n",
    "    \"\"\"\n",
    "    ax.plot(np.arange(len(steps)), steps, marker='o', color='C0', label='Steps to Go')\n",
    "    ax.axhline(optimal_steps, color='C2', label='Optimal Steps')\n",
    "    ax.set_xlabel('Episodes')\n",
    "    ax.set_ylabel('Steps')\n",
    "    ax.set_xlim(left=0, right=max_episodes)\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "def plot_state_value_heatmap(ax, V, fig, title: str, color_vmin: int = -15, color_vmax: int = 0):\n",
    "    \"\"\"Plot state value as a heatmap.\n",
    "\n",
    "    :param ax: An axis object of matplotlib.\n",
    "    :param V: The final estimated state values.\n",
    "    :param fig: The fig parameter, acquired when calling plt.subplots().\n",
    "    :param title: Title for the state value heatmap plot.\n",
    "    :param color_vmin: Min value for the color map indicator, default to -15.\n",
    "    :param color_vmax: Max value for the color map indicator, default to 0.\n",
    "    \"\"\"\n",
    "    hm = ax.imshow(\n",
    "        V,\n",
    "        cmap='hot',\n",
    "        interpolation='nearest',\n",
    "        vmin=color_vmin,  # colorbar min\n",
    "        vmax=color_vmax,  # colorbar max\n",
    "    )\n",
    "    # cbar = fig.colorbar(hm)\n",
    "    # cbar.set_label('State Value')\n",
    "\n",
    "    m, n = V.shape\n",
    "    for i in range(m):  # add V value to each heatmap cell\n",
    "        for j in range(n):\n",
    "            val = '-inf' if V[i, j] <= -1000.0 else f'{V[i, j]:.2f}'\n",
    "            ax.annotate(val, xy=(j - 0.5, i + 0.1), fontsize=10)\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "def plot_path(\n",
    "    ax,\n",
    "    actor,\n",
    "    start: Tuple[int, int],\n",
    "    end: Tuple[int, int],\n",
    "    title: str,\n",
    "    max_steps: int,\n",
    "    rng,\n",
    "    grid,\n",
    "):\n",
    "    \"\"\"Plot a route from start to end on the grid world.\n",
    "\n",
    "    Yellow is start; green is end; brown is obstacle.\n",
    "\n",
    "    :param ax: An axis object of matplotlib.\n",
    "    :param actor: The policy function NN simulation.\n",
    "    :param start: The start state in the format of (row_idx, col_idx)\n",
    "    :param end: The end state in the format of (row_idx, col_idx)\n",
    "    :param title: Title for the path plot.\n",
    "    :param max_steps: Max number of steps allowed when drawing a path. If\n",
    "        max steps are reached, the path is left as is.\n",
    "    :param rng: A random number generator.\n",
    "    :param grid: The grid world. Default to GRID.\n",
    "    \"\"\"\n",
    "    m, n = grid.shape\n",
    "    ax.grid(True)\n",
    "    ax.set_xticks(np.arange(n))\n",
    "    ax.set_xlim(left=0, right=n)\n",
    "    ax.set_yticks(np.arange(m))\n",
    "    ax.set_ylim(top=m - 1, bottom=-1)\n",
    "    ax.invert_yaxis()  # invert y axis such that 0 is at the top\n",
    "    i, j = start\n",
    "    step = 0  # note that each invalid action counts as a step as well.\n",
    "    while (i, j) != end and step < max_steps:\n",
    "        a, _ = next_action(i, j, actor, rng=rng)\n",
    "        di, dj = ACTIONS[a]\n",
    "        ni, nj = i + di, j + dj\n",
    "        if is_valid_state(ni, nj, grid=grid):  \n",
    "            ax.plot(  # jiggle on each path plot to visualize paths taken multiple times vs. once\n",
    "                [j + 0.5 + (rng.random() - 0.5) / 10, nj + 0.5 + (rng.random() - 0.5) / 10],\n",
    "                [i - 0.5 + (rng.random() - 0.5) / 10, ni - 0.5 + (rng.random() - 0.5) / 10],\n",
    "                color='red',\n",
    "                lw=2,\n",
    "            )\n",
    "            i, j = ni, nj\n",
    "        step += 1\n",
    "    # label start, end and blocks\n",
    "    ax.add_patch(Rectangle((0, -1), 1, 1, fill=True, color='yellow'))\n",
    "    ax.add_patch(Rectangle((n - 1, m - 2), 1, 1, fill=True, color='green'))\n",
    "    for i, j in zip(*np.where(grid == 1)):\n",
    "        ax.add_patch(Rectangle((j, i - 1), 1, 1, fill=True, color='brown'))\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "def plot_loss(ax, critic_losses: List, actor_losses: List, title: str, max_episodes: int):\n",
    "    \"\"\"Plot critic and actor loss in a single graph.\n",
    "\n",
    "    Critic loss takes the left y-axis. Actor loss takes the right.\n",
    "\n",
    "    :param ax: An axis object of matplotlib.\n",
    "    :param critic_losses: A list of critic loss per episode.\n",
    "    :param actor_losses: A list of actor loss per episode.\n",
    "    :param title: Title for the path plot.\n",
    "    :param max_episodes: The maximum number of episodes.\n",
    "    \"\"\"\n",
    "    num_loss = len(critic_losses)\n",
    "    twin = ax.twinx()  # put actor loss on a separate y axis\n",
    "    critic_plot, = ax.plot(np.arange(num_loss), critic_losses, color='C0', label='Critic')\n",
    "    actor_plot, = twin.plot(np.arange(num_loss), actor_losses, color='C1', label='Actor')\n",
    "    ax.set_xlim(left=0, right=max_episodes)\n",
    "    # configure critic loss axis\n",
    "    ax.set_ylabel('Critic Loss')\n",
    "    ax.yaxis.label.set_color(critic_plot.get_color())\n",
    "    ax.tick_params(axis='y', colors=critic_plot.get_color())\n",
    "    # configure actor loss axis\n",
    "    twin.set_ylabel('Actor Loss')\n",
    "    twin.yaxis.label.set_color(actor_plot.get_color())\n",
    "    twin.tick_params(axis='y', colors=actor_plot.get_color())\n",
    "    ax.set_xlabel('Episodes')\n",
    "    ax.set_title(title)\n",
    "    ax.legend(handles=[critic_plot, actor_plot])\n",
    "\n",
    "\n",
    "def plot(\n",
    "    title: str,\n",
    "    critic,\n",
    "    actor,\n",
    "    critic_losses: List,\n",
    "    actor_losses: List,\n",
    "    steps: List,\n",
    "    start: Tuple[int, int],\n",
    "    end: Tuple[int, int],\n",
    "    max_steps: int,\n",
    "    max_episodes: int,\n",
    "    grid,\n",
    "    V,\n",
    "    optimal_steps: int = OPTIMAL_STEPS,\n",
    "):\n",
    "    \"\"\"One-stop shop to plot all performance metrics during training.\n",
    "\n",
    "    :param title: The suptitle of the entire figure.\n",
    "    :param critic: The state value function NN simulation.\n",
    "    :param actor: The policy function NN simulation.\n",
    "    :param critic_losses: A list of critic losses per episode.\n",
    "    :param actor_losses: A list of actor loss per episode.\n",
    "    :param steps: A list of steps taken per episode. A failed episode has the max\n",
    "        number of steps allowed.\n",
    "    :param start: The start state in the format of (row_idx, col_idx)\n",
    "    :param end: The end state in the format of (row_idx, col_idx)\n",
    "    :param max_steps: Maximum number of steps per episode.\n",
    "    :param max_episodes: Maximum number of episodes.\n",
    "    :param grid: The grid world. Default to GRID.\n",
    "    :param V: The final estimated state values.\n",
    "    :param optimal_steps: The optimal number of steps to reach the target.\n",
    "    \"\"\"\n",
    "    rng=np.random.default_rng()\n",
    "    plt.rc('grid', linestyle=\"-\", color='black')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 13))\n",
    "    axes = axes.flatten()\n",
    "    # First three plots\n",
    "    plot_step_to_go(axes[0], steps, optimal_steps, max_episodes, '(A)')\n",
    "    plot_loss(axes[1], critic_losses, actor_losses, '(B)', max_episodes)\n",
    "    plot_path(axes[2], actor, start, end, '(C)', max_steps, rng, grid=grid)\n",
    "    # Compute V table\n",
    "    for i in range(grid.shape[0]):\n",
    "        for j in range(grid.shape[1]):\n",
    "            V[i, j] = -1000 if grid[i, j] else critic(state(i, j))[0, 0]\n",
    "    V[grid.shape[0] - 1, grid.shape[1] - 1] = 0\n",
    "    plot_state_value_heatmap(axes[3], V, fig, '(D)', color_vmin=-15, color_vmax=-3)\n",
    "    \n",
    "    fig.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    filename = '_'.join(title.split())\n",
    "    if filename:\n",
    "        plt.savefig(filename + '.png')\n",
    "        plt.ioff()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "source": [
    "# Initialize Grid and State Value Table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_grid(num_rows: int, num_cols: int, num_blocks: int, random_state: int = RANDOM_SEED):\n",
    "    \"\"\"Initialize a grid world, with obstacles.\n",
    "\n",
    "    :param num_rows: Number of rows in the grid world.\n",
    "    :param num_cols: Number of columns in the grid world.\n",
    "    :param num_blocks: Number of RANDOM blocks that agent cannot cross. The final grid world\n",
    "        could include more blocks as additional  a manually \n",
    "    :param random_state: For reproducing random values.\n",
    "    :return: The grid itself and the tabulation for state values.\n",
    "    \"\"\"\n",
    "    # Create a grid\n",
    "    grid = np.array([[0] * num_cols for _ in range(num_rows)])\n",
    "    V = np.array([[0.0] * num_cols for _ in range(num_rows)])\n",
    "    # randomly generate blocks\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    block_i = rng.integers(low=0, high=num_rows, size=num_blocks)\n",
    "    block_j = rng.integers(low=0, high=num_cols, size=num_blocks)\n",
    "    \n",
    "    ###################################################\n",
    "    # hardcodede additional blocks, such that we can  #\n",
    "    # limit the possible number of ways to reach the  #\n",
    "    # terminal state                                  #\n",
    "    ###################################################\n",
    "    block_i = np.append(block_i, [8, 8, 8])\n",
    "    block_j = np.append(block_j, [4, 5, 6])\n",
    "    \n",
    "    # Update grid and state values by identifying the blocks\n",
    "    grid[block_i, block_j] = 1\n",
    "    return grid, V\n",
    "\n",
    "\n",
    "GRID, V = initialize_grid(M, N, NUM_BLK, random_state=10)"
   ]
  },
  {
   "source": [
    "# Training Helper Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_state(i: int, j: int, a: int, grid=GRID) -> Tuple[int, int]:\n",
    "    \"\"\"Obtain the next state given the current state and action.\n",
    "\n",
    "    The peculiarity of this is that if the next state is invalid, i.e.\n",
    "    it is easier outside the grid world or in a blockage, we return the\n",
    "    original state, signifying that the result of the illegal action\n",
    "    results in the agent being stuck at the same location.\n",
    "\n",
    "    :param i: Row index of the agent on the grid world.\n",
    "    :param j: Column index of the agent on the grid world.\n",
    "    :param a: Index of ACTIONS, signifying the specific action to take.\n",
    "    :param grid: The grid world. Default to GRID.\n",
    "    :return: The next cell's row and column index.\n",
    "    \"\"\"\n",
    "    di, dj = ACTIONS[a]\n",
    "    ni, nj = i + di, j + dj\n",
    "    if is_valid_state(ni, nj, grid=grid):\n",
    "        return ni, nj\n",
    "    return i, j\n",
    "\n",
    "\n",
    "def next_action(i: int, j: int, actor, rng) -> Tuple[float, float]:\n",
    "    \"\"\"Obtain the next action from the publicy function simulation.\n",
    "\n",
    "    Note that we also obtain the probability of the action as reported\n",
    "    by the actor.\n",
    "\n",
    "    :param i: Row index of the agent on the grid world.\n",
    "    :param j: Column index of the agent on the grid world.\n",
    "    :param actor: The policy function simulation.\n",
    "    :returns: The action and its associated probability.\n",
    "    \"\"\"\n",
    "    a = rng.choice(len(ACTIONS), p=np.squeeze(actor(state(i, j))))\n",
    "    return a, actor(state(i, j))[0, a]\n",
    "\n",
    "\n",
    "def state(i: int, j: int):\n",
    "    \"\"\"Generate a numpy array version of the state.\n",
    "\n",
    "    This is used as the input value for the state and policy function\n",
    "    NN.\n",
    "\n",
    "    :param i: Row index of the agent on the grid world.\n",
    "    :param j: Column index of the agent on the grid world.\n",
    "    :return: A numpy array of shape (1, 2)\n",
    "    \"\"\"\n",
    "    return np.array([[i, j]])\n",
    "\n",
    "\n",
    "def state_action(i: int, j: int, a: int):\n",
    "    \"\"\"Generate a numpy array version of the state action pair.\n",
    "\n",
    "    This is used as the input value for the action value function NN.\n",
    "\n",
    "    :param i: Row index of the agent on the grid world.\n",
    "    :param j: Column index of the agent on the grid world.\n",
    "    :param a: Index of ACTIONS, signifying the specific action to take.\n",
    "    :return: A numpy array of shape (1, 3)\n",
    "    \"\"\"\n",
    "    return np.array([[i, j, a]])\n",
    "\n",
    "\n",
    "def is_valid_state(i: int, j: int, grid=GRID) -> bool:\n",
    "    \"\"\"Check whether the current state is valid.\n",
    "\n",
    "    A valid state must be within the grid world and NOT on any of the\n",
    "    blocks.\n",
    "\n",
    "    :param i: Row index of the agent on the grid world.\n",
    "    :param j: Column index of the agent on the grid world.\n",
    "    :param grid: The grid world. Default to GRID.\n",
    "    :return: A boolean value, true => state is valid, false otherwise.\n",
    "    \"\"\"\n",
    "    m, n = grid.shape\n",
    "    return 0 <= i < m and 0 <= j < n and grid[i, j] == 0"
   ]
  },
  {
   "source": [
    "# Actor Critic Architecture"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN_SQUARED_ERROR = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "def actor_critic_monte_carlo(\n",
    "    actor,\n",
    "    critic,\n",
    "    grid,\n",
    "    V,\n",
    "    start: Tuple[int, int],\n",
    "    end: Tuple[int, int],\n",
    "    include_all: bool = True,\n",
    "    max_episodes: int = 200,\n",
    "    max_steps: int = 1000,\n",
    "    actor_opt = ACTOR_OPTIMIZER,\n",
    "    critic_opt = CRITIC_OPTIMIZER,\n",
    "    gamma: float = GAMMA,\n",
    "    random_seed: int = RANDOM_SEED,\n",
    "):\n",
    "    \"\"\"Find the shortest path in the grid world using actor critic architecture with Monte Carlo.\n",
    "\n",
    "    Since Monte Carlo is used, each episode extends until the terminal state is reached or the\n",
    "    max number of steps achieved, whichever comes first. Based on the path, we can compute the\n",
    "    actual discounted reward at each step, and use that to compute the error on critic. This\n",
    "    error is then used to compute the loss function for both the critic and actor NN for gradient\n",
    "    descent/ascent optimization.\n",
    "\n",
    "    :param actor: The policy function NN simulation.\n",
    "    :param critic: The state value function NN simulation.\n",
    "    :param grid: The grid world.\n",
    "    :param V: The tabular state values. For use in the plot only.\n",
    "    :param start: The grid coordinates of the starting position.\n",
    "    :param end: The grid coordinates of the end position.\n",
    "    :param include_all: A boolean flag to determine whether all episodes, successful or failed,\n",
    "        are included in the training data. Default to True.\n",
    "    :param max_episodes: Maxinum episodes to use in training the critic and actor. If include_all\n",
    "        is set to False, significantly more episodes might be needed overall to achieve max_episodes\n",
    "        number of successful expisodes. Default to 200.\n",
    "    :param max_steps: Maximum steps to take in the grid world in each episode.\n",
    "        Default to 1000.\n",
    "    :param actor_opt: Optimizer for the actor NN. Default to ACTOR_OPTIMIZER.\n",
    "    :param critic_opt: Optimizer for the critic NN. Default to CRITIC_OPTIMIZER.\n",
    "    :param gamma: Discount rate. Default to GAMMA.\n",
    "    :param random_seed: To seed a random number generator.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_seed)\n",
    "    m, n = grid.shape\n",
    "    all_critic_losses = []\n",
    "    all_actor_losses = []\n",
    "    all_steps = []\n",
    "    ep = 0\n",
    "    total_ep = 0\n",
    "    while ep < max_episodes:\n",
    "        critic_vals = []\n",
    "        actor_log_probs = []\n",
    "        rewards = []\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            i, j = start\n",
    "            step = 0  # record steps taken in the current episode\n",
    "            while step < max_steps and (i, j) != end:\n",
    "                a, aprob = next_action(i, j, actor, rng=rng)\n",
    "                cval = critic(state(i, j))[0, 0]\n",
    "                critic_vals.append(cval)\n",
    "                actor_log_probs.append(tf.math.log(aprob))\n",
    "                rewards.append(-1)  # reward is always -1.\n",
    "                i, j = next_state(i, j, a, grid=grid)\n",
    "                step += 1\n",
    "            # Compute discounted returns\n",
    "            returns = []\n",
    "            discounted_return = 0\n",
    "            for r in rewards[::-1]:\n",
    "                discounted_return = r + gamma * discounted_return\n",
    "                returns.append(discounted_return)\n",
    "            returns = returns[::-1]\n",
    "            # Convert to Tensors\n",
    "            critic_vals = tf.convert_to_tensor(critic_vals, dtype=tf.float32)\n",
    "            actor_log_probs = tf.convert_to_tensor(actor_log_probs, dtype=tf.float32)\n",
    "            returns = tf.convert_to_tensor(returns, dtype=tf.float32)\n",
    "            # Compute critic and actor NN loss\n",
    "            advtange = returns - critic_vals\n",
    "            actor_loss = -tf.math.reduce_mean(actor_log_probs * advtange)\n",
    "            critic_loss = MEAN_SQUARED_ERROR(critic_vals, returns)\n",
    "\n",
    "        # Compute gradient and update NN weights\n",
    "        # However, NN weights update ONLY if the current episode is successful, or\n",
    "        # it is instructed to include all expisodes\n",
    "        if step < max_steps or include_all:\n",
    "            actor_gradient = tape.gradient(actor_loss, actor.trainable_variables)\n",
    "            actor_opt.apply_gradients(zip(actor_gradient, actor.trainable_variables))\n",
    "            critic_gradient = tape.gradient(critic_loss, critic.trainable_variables)\n",
    "            critic_opt.apply_gradients(zip(critic_gradient, critic.trainable_variables))\n",
    "            ep += 1\n",
    "            # Record losses and steps\n",
    "            all_critic_losses.append(critic_loss)\n",
    "            all_actor_losses.append(actor_loss)\n",
    "            all_steps.append(step)\n",
    "            if ep % 10 == 0:\n",
    "                print(f'Episode: {ep}, critic loss: {critic_loss}, actor loss: {actor_loss}', end=' ')\n",
    "                if (i, j) == end:\n",
    "                    print(f'Terminal State, steps: {step}')\n",
    "                else:\n",
    "                    print('Max steps exhausted')\n",
    "                plot(\n",
    "                    f'Ep {(ep):03} Summary',\n",
    "                    critic,\n",
    "                    actor,\n",
    "                    all_critic_losses,\n",
    "                    all_actor_losses,\n",
    "                    all_steps,\n",
    "                    start,\n",
    "                    end,\n",
    "                    max_steps,\n",
    "                    max_episodes,\n",
    "                    grid,\n",
    "                    V,\n",
    "                )\n",
    "        total_ep += 1\n",
    "    print(f'Total episodes: {total_ep}')"
   ]
  },
  {
   "source": [
    "# Grid World with Two Openings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the actor critic architecture\n",
    "policy_model_1 = policy_func(128, 1, len(ACTIONS))\n",
    "state_value_model_1 = state_value_func(128, 1)\n",
    "actor_critic_monte_carlo(\n",
    "    policy_model_1,\n",
    "    state_value_model_1,\n",
    "    GRID,\n",
    "    V,\n",
    "    (0, 0),\n",
    "    (GRID.shape[0] - 1, GRID.shape[1] - 1),\n",
    ")\n",
    "# Save trained models\n",
    "policy_model_1.save('policy_model_1')\n",
    "state_value_model_1.save('state_value_model_1')"
   ]
  },
  {
   "source": [
    "# Grid World with One Opening"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Top left to bottom right"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add one more block such that there is only one opening to the terminal state\n",
    "GRID[7, 7] = 1\n",
    "\n",
    "# Train the actor critic architecture\n",
    "policy_model_3 = policy_func(128, 1, len(ACTIONS))\n",
    "state_value_model_3 = state_value_func(128, 1)\n",
    "actor_critic_monte_carlo(\n",
    "    policy_model_3,\n",
    "    state_value_model_3,\n",
    "    GRID,\n",
    "    V,\n",
    "    (0, 0),\n",
    "    (GRID.shape[0] - 1, GRID.shape[1] - 1),\n",
    ")\n",
    "# Save trained models\n",
    "policy_model_3.save('policy_model_3')\n",
    "state_value_model_3.save('state_value_model_3')\n",
    "\n",
    "# Remove the block\n",
    "GRID[7, 7] = 0"
   ]
  },
  {
   "source": [
    "## Bottom right to top left"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add one more block such that there is only one opening to the terminal state\n",
    "GRID[7, 7] = 1\n",
    "\n",
    "# Train the actor critic architecture\n",
    "policy_model_4 = policy_func(128, 1, len(ACTIONS))\n",
    "state_value_model_4 = state_value_func(128, 1)\n",
    "actor_critic_monte_carlo(\n",
    "    policy_model_4,\n",
    "    state_value_model_4,\n",
    "    GRID,\n",
    "    V,\n",
    "    (GRID.shape[0] - 1, GRID.shape[1] - 1),\n",
    "    (0, 0),\n",
    ")\n",
    "# Save trained models\n",
    "policy_model_4.save('policy_model_4')\n",
    "state_value_model_4.save('state_value_model_4')\n",
    "\n",
    "# Remove the block\n",
    "GRID[7, 7] = 0"
   ]
  }
 ]
}